https://github.com/lehuyduc/1brc-simd

`main.cpp` and `main_small.cpp` follows all the requirements specified by the challenges (no preprocessing, has to work with all valid inputs length <= 100, no extra input assumption, single-file no libraries, etc).

Tested on file size 13795495846 bytes, generated by `./create_measurements.sh 1000000000`. See my repo for link to both default and 10K datasets. Tested on many PCs. No HugeTLB.

**Threadripper PRO 5995WX 64c128t, 256GB DDR4 unknown speed**
Ubuntu 20.04, virtual machine, g++ 9.4. All very detailed results are in `benchmark_results` folder in my repo.

`Bandwidth = 6.47131e+10 byte/s` tested using `test_bandwidth.sh`. To ensure benchmark are comparable, you must check not just the CPU but also RAM bandwidth. Also the dataset used can make a big difference.

Version: `1brc_valid23_small`
![image](https://github.com/gunnarmorling/1brc/assets/22568157/d3f7bada-bc9b-4fd0-a6d6-319dc926ebe1)
```
root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 ./main
Benchmark 1: ./main
  Time (mean Â± Ïƒ):     179.3 ms Â±   8.0 ms    [User: 1.2 ms, System: 1.2 ms]
  Range (min â€¦ max):   166.0 ms â€¦ 191.2 ms    10 runs
```

To get the best number possible, you will need luck. Run your code a bunch of time and pray for a good result (it's just like playing the slot machine ðŸ˜† ). I think I ran it ~20 times or so, initially with debug info on but had to turn it off to save some extra milliseconds. `hyperfine` will make the code slower, so just run manually.

`_small` suffix means it will still work with all inputs, but is slower on the 10K dataset (1.5-2x slower than `1brc_valid23`). This is because the hash table size is only `16384`, which is the smallest size > number of unique keys.

Version: `1brc_valid23`, default dataset
```
root@C.8621043:~/1brc-simd$ ./run.sh 128 128
Using 128 threads
PC has 128 physical cores
Malloc cost = 0.014787
init mmap file cost = 0.030437ms
n_threads = 128
Gather key stats cost = 0.001603
Parallel process file cost = 173.421ms
Aggregate stats cost = 11.7294ms
Output stats cost = 0.590497ms
Runtime inside main = 185.911ms
real    0m0.189s
user    0m0.002s
sys     0m0.000s

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 ./main
Benchmark 1: ./main
  Time (mean Â± Ïƒ):     210.6 ms Â±   4.6 ms    [User: 2.1 ms, System: 0.2 ms]
  Range (min â€¦ max):   204.6 ms â€¦ 216.8 ms    10 runs

//---------------------
root@C.8621043:~/1brc-simd$ ./run.sh 
Using 128 threads
PC has 64 physical cores
Malloc cost = 0.018525
init mmap file cost = 0.031208ms
n_threads = 128
Gather key stats cost = 4.43294
Parallel process file cost = 175.646ms
Aggregate stats cost = 12.7642ms
Output stats cost = 0.675469ms
Runtime inside main = 193.694ms
real    0m0.197s
user    0m0.000s
sys     0m0.002s
a55d0d9d02661c33538f2e11bb86f1825a5f015d6dd3645416ec71bc50099ee5  result.txt

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 ./main
Benchmark 1: ./main
  Time (mean Â± Ïƒ):     214.6 ms Â±   4.3 ms    [User: 1.9 ms, System: 0.4 ms]
  Range (min â€¦ max):   206.3 ms â€¦ 219.8 ms    10 runs

//---------------------
root@C.8621043:~/1brc-simd$ ./run.sh 8
Using 8 threads
PC has 64 physical cores
Malloc cost = 0.018164
init mmap file cost = 0.030487ms
n_threads = 8
Gather key stats cost = 0.001663
Parallel process file cost = 1095.17ms
Aggregate stats cost = 3.9564ms
Output stats cost = 0.942773ms
Runtime inside main = 1100.23ms
real    0m1.103s
user    0m0.003s
sys     0m0.000s

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 ./main
Benchmark 1: ./main
  Time (mean Â± Ïƒ):      1.131 s Â±  0.006 s    [User: 0.002 s, System: 0.001 s]
  Range (min â€¦ max):    1.125 s â€¦  1.143 s    10 runs
```

Version: `1brc_valid23`, 10K dataset
```
root@C.8621043:~/1brc-simd$ ./run.sh 128 128
Using 128 threads
PC has 128 physical cores
Malloc cost = 0.018836
init mmap file cost = 0.029576ms
n_threads = 128
Gather key stats cost = 0.001643
Parallel process file cost = 261.406ms
Aggregate stats cost = 41.1531ms
Output stats cost = 13.2542ms
Runtime inside main = 316.002ms
real    0m0.319s
user    0m0.003s
sys     0m0.000s
e99d23f6fa210b0d9c43a63e335d8d49f4a247ca7cc237bea0fe4c8b64b1933e  result.txt

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 './main measurements_10k.txt'
Benchmark 1: ./main measurements_10k.txt
  Time (mean Â± Ïƒ):     362.6 ms Â±  11.4 ms    [User: 1.7 ms, System: 0.9 ms]
  Range (min â€¦ max):   347.1 ms â€¦ 384.0 ms    10 runs

//---------------------
root@C.8621043:~/1brc-simd$ ./run.sh 
Using 128 threads
PC has 64 physical cores
Malloc cost = 0.022262
init mmap file cost = 0.033132ms
n_threads = 64
Gather key stats cost = 4.60424
Parallel process file cost = 313.789ms
Aggregate stats cost = 32.4718ms
Output stats cost = 10.7591ms
Runtime inside main = 361.798ms
real    0m0.365s
user    0m0.000s
sys     0m0.003s
e99d23f6fa210b0d9c43a63e335d8d49f4a247ca7cc237bea0fe4c8b64b1933e  result.txt

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 './main measurements_10k.txt'
Benchmark 1: ./main measurements_10k.txt
  Time (mean Â± Ïƒ):     361.3 ms Â±   5.8 ms    [User: 1.5 ms, System: 1.0 ms]
  Range (min â€¦ max):   350.0 ms â€¦ 370.9 ms    10 runs

//---------------------
root@C.8621043:~/1brc-simd$ ./run.sh 8
Using 8 threads
PC has 64 physical cores
Malloc cost = 0.017142
init mmap file cost = 0.03174ms
n_threads = 8
Gather key stats cost = 0.001583
Parallel process file cost = 1339.21ms
Aggregate stats cost = 17.3071ms
Output stats cost = 10.1368ms
Runtime inside main = 1366.83ms
real    0m1.370s
user    0m0.002s
sys     0m0.000s
e99d23f6fa210b0d9c43a63e335d8d49f4a247ca7cc237bea0fe4c8b64b1933e  result.txt

root@C.8621043:~/1brc-simd$ hyperfine --warmup 1 --runs 10 './main measurements_10k.txt'
Benchmark 1: ./main measurements_10k.txt
  Time (mean Â± Ïƒ):      1.413 s Â±  0.018 s    [User: 0.002 s, System: 0.001 s]
  Range (min â€¦ max):    1.391 s â€¦  1.443 s    10 runs
```

**At 128 threads, the program finishes the challenge faster than the OS can `munmap` the input file.** `munmap` alone takes up over 50% of the time, so it cost more than the program's allocation, initialization, processing, output, and freeing resources all combined. **So, I use the subprocess trick used by top Java solutions**, basically bypassing `munmap` and memory deallocation. It's possible to do this the correct way by either using `fork()` instead of threads, or let each thread process different input sizes and `munmap` at different times.

Running using `hyperfine` is slower than running the command manually, as the previous `munmap` hasn't finished.

With this many threads, aggregating the results from all threads take a noticeable amount of time. So we use parallel processing to speed up that too, saving ~28ms at 128 threads. By using 2 layers of parallel aggregation, we save an extra ~2.5ms lol. 

--------------------
**Found a PC to borrow with the same spec as the official test server (AMD EPYC 7502 CPU, 256GB RAM).** So I also tested top 3 solutions (lowest 10K dataset time on the official leaderboard). For each person, I tested 2 versions: with native binary, and without. I use the code from the official repo, commit 673c1b9

**Results are currently outdated so I've removed them.** In my repo you can see the old results in `benchmark_results/old_post2.txt`, and files `other_artsi_7502p`, `other_thomas_7502p`, `other_royvanrijn_7502p`, `valid20_7502p`, `valid20_7502p_10k`

**Other submission**
There's a super fast non-compliant solution at: https://curiouscoding.nl/posts/1brc/ 
It doesn't work with all inputs like required, but is extremely fast and has very creative ideas.


There's a super fast compliant version using .NET at: https://github.com/noahfalk/1brc/tree/main
I tested it on **EPYC 7502** (5995WX PC was not available at the time). This code doesn't use `mmap`, so its `hyperfine` result is similar to running manually. See files: `other_noahfalk`, `valid23_7502p.txt`, `valid23_7502p_10k.txt`

Below is a lot of raw data, so here's the summary:
| Name/Test  | Original, 1 thread | Original, 8 thread | Original, best thread | 10K, 1 thread | 10K, 8 thread | 10K, best thread |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| lehuyduc  | 10.720s  | 1.358s  | **0.295s**  | **14.095s** | **1.810s**  | **0.586s** |
| noahfalk  | **7.060**  | **0.926s**  | 0.376s  | 20.882 | 2.719 | 0.994s |

@noahfalk solution use an extreme amount of manual SIMD and ILP tricks, so it performs much better at lower thread counts. I'm not sure why it performs worse at higher thread counts yet. For example 32 vs 64:
```
root@C.8687809:~/1brc/1brc/bin/Release/net8.0/linux-x64/publish$ time ./1brc /root/1brc-simd/measurements.txt --threads 32 > log.txt
real    0m0.376s
user    0m7.530s
sys     0m1.878s

root@C.8687809:~/1brc/1brc/bin/Release/net8.0/linux-x64/publish$ time ./1brc /root/1brc-simd/measurements.txt > log.txt
real    0m0.470s
user    0m12.984s
sys     0m2.584s
```

------------------------------------------------------
------------------------------------------------------
------------------------------------------------------

**Default dataset, noahfalk**
32 threads (better than 64 threads, tested both)
![image](https://github.com/gunnarmorling/1brc/assets/22568157/43c28c1c-b001-4d8a-a5ed-465269b32b77)
```
root@C.8687809:~/1brc/1brc/bin/Release/net8.0/linux-x64/publish$ hyperfine -w 1 -r 10 -- "./1brc /root/1brc/measurements.txt --threads 32 > log.txt"
Benchmark 1: ./1brc /root/1brc/measurements.txt --threads 32 > log.txt
  Time (mean Â± Ïƒ):     390.9 ms Â±  10.3 ms    [User: 7679.4 ms, System: 1823.0 ms]
  Range (min â€¦ max):   367.8 ms â€¦ 403.7 ms    10 runs
```

8 threads and 1 thread:
![image](https://github.com/gunnarmorling/1brc/assets/22568157/9ee5b430-a47d-48d8-b290-c1d13f8795f6)


**Default dataset, 1brc_valid23**
64 threads 
![image](https://github.com/gunnarmorling/1brc/assets/22568157/9f3ddb41-d927-4bbc-bc73-2a147c740a0c)

Running with `hyperfine` slows my code down by ~70ms

8 threads and 1 thread:
```
Using 8 threads
...
real    0m1.358s
user    0m0.003s
sys     0m0.000s

Using 1 threads
...
real    0m10.720s
user    0m0.003s
sys     0m0.000s
```


**10K dataset, noahfalk**
32 threads (faster than 64, tested both)
```
root@C.8687809:~/1brc/1brc/bin/Release/net8.0/linux-x64/publish$ hyperfine -w 1 -r 5 -- "./1brc /root/1brc-simd/measurements_10k.txt --threads 32 > log.txt"
Benchmark 1: ./1brc /root/1brc-simd/measurements_10k.txt --threads 32 > log.txt
  Time (mean Â± Ïƒ):      1.026 s Â±  0.008 s    [User: 22.361 s, System: 4.271 s]
  Range (min â€¦ max):    1.018 s â€¦  1.038 s    5 runs
```

8 threads and 1 thread:
![image](https://github.com/gunnarmorling/1brc/assets/22568157/ede27bf9-05a0-4186-893e-3052dbada00a)

**10K dataset, 1brc_valid23**
64 threads:
![image](https://github.com/gunnarmorling/1brc/assets/22568157/32642944-6200-4fd8-b97d-f822bb3653b1)
```
root@C.8687809:~/1brc-simd$ hyperfine --warmup 1 --runs 10 "./main measurements_10k.txt"
Benchmark 1: ./main measurements_10k.txt
  Time (mean Â± Ïƒ):     694.9 ms Â±  24.1 ms    [User: 2.0 ms, System: 1.2 ms]
  Range (min â€¦ max):   652.9 ms â€¦ 716.9 ms    10 runs
```
With 10K dataset, my `hyperfine` result is ~100ms slower.

8 threads and 1 thread:
```
Using 8 threads
...
real    0m1.810s
user    0m0.000s
sys     0m0.002s

Using 1 threads
...
real    0m14.095s
user    0m0.000s
sys     0m0.002s
```
------------------------------------------------------
------------------------------------------------------
------------------------------------------------------

**Main ideas:**
------------------------------------
- Unsigned int overflow hashing: cheapest hash method possible.
- SIMD to find multiple separator `;` and loop through them
- SIMD hashing
- SIMD for string comparison in hash table probing
- ILP trick (each thread process 2 different lines at a time)
- Parse number as int instead of float 
- Notice properties of actual data
- + 99% of station names has `length <= 16`, use compiler hint + implement SIMD for this specific case. If length > 16, use a fallback => still meet requirements of `MAX_KEY_LENGTH = 100`
- + `-99.9 <= temperature <= 99.9` guaranteed, use special code using this property
- Use mmap for fast file reading
- Use multithreading for both parsing the file, and aggregating the data
- Other random tricks (intentional ordering of variable assignments)
- **Automatic RAM disk**: the contest assumes the file is in RAM, not disk. This is done automatically on Linux if you read the file using `mmap` once, then run the program again. Without this rule, all solutions will be completely different.

**Others**
~~There's a potential out-of-range-access exploit in the code. It's left as exercise for the reader.~~ 

I optimize the code for hyper threading. For example with a 16c32t CPU, if a change improves performance when running 32 threads but slightly decreases performance at 16, I will keep that change. Disabling HT will increase performance at the same thread count (maybe even 20-25%), so there are some situations where it's the correct choice.