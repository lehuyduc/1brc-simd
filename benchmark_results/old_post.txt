https://github.com/lehuyduc/1brc-simd

`1brc_final_valid.cpp` follows all the requirements specified by the challenges (no preprocessing, has to work with all valid inputs length <= 100, no extra input assumption, single-file no libraries, etc).

**Summary comparison vs official leaderboard** (left tested on AMD 2950X 16c32t, right tested on AMD 7502P 32c64t)
- Default dataset, 8 thread: 1.356s vs 2.552s
- Default dataset, **32 thread vs 64 thread**: 0.587s vs 0.799s
- 10K dataset, 8 thread: 2.266s vs 4.589s

Tested on file size 13795495846 bytes, generated by `./create_measurements.sh 1000000000`. Tested on many PCs. No HugeTLB.

~**Dual Socket AMD EPYC 9354 3.75 GHz (64c128t total), 768GB 12-channel DDR5 3200MHz**~
**Threadripper PRO 5995WX 64c128t, 256GB DDR4 unknown speed**
Ubuntu 20.04, virtual machine, g++ 9.4. 
Test version: `1brc_valid16`
**Finally broke through 0.4 second total time barrier!!!** Thanks to `munmap` time on 5995WX PC being much faster than on EPYC 9354 PC (190ms vs 220ms). 

```
Using 128 threads
Malloc cost = 0.006652
init mmap file cost = 0.014066ms
Parallel process file cost = 152.155ms
Aggregate stats cost = 2.44852ms
Output stats cost = 0.598279ms
Runtime inside main = 155.241ms
Time to munmap = 194.04
Time to free memory = 9.50263
real    0m0.362s
user    0m15.564s
sys     0m1.081s

Using 64 threads
Malloc cost = 0.006061
init mmap file cost = 0.014647ms
Parallel process file cost = 231.558ms
Aggregate stats cost = 2.32858ms
Output stats cost = 1.14539ms
Runtime inside main = 235.081ms
Time to munmap = 196.901
Time to free memory = 4.66092
real    0m0.440s
user    0m12.055s
sys     0m0.899s

Using 32 threads
Malloc cost = 0.006924
init mmap file cost = 0.020569ms
Parallel process file cost = 386.718ms
Aggregate stats cost = 1.39825ms
Output stats cost = 0.558864ms
Runtime inside main = 388.708ms
Time to munmap = 193.599
Time to free memory = 3.01529
real    0m0.588s
user    0m11.753s
sys     0m0.642s
```

**At 128 threads, the program finishes the challenge faster than the OS can `munmap` the input file.** `munmap` alone takes up over 50% of the time, so it cost more than the program's allocation, initialization, processing, output, and freeing resources all combined. 

With this many threads, aggregating the results from all threads take a noticeable amount of time. So we use parallel processing to speed up that too, saving ~28ms at 128 threads. By using 2 layers of parallel aggregation, we save an extra ~2.5ms lol. 

**AMD 2950X (16c32t), quad channel 2133MHz RAM**, test version `1brc_valid19_small`, default dataset
Versions with `_small` suffix will still work with all inputs, but it will be much slower on 10k keys datasets, because the hash table size is set to only `16384` instead of  `16384 * 8`.
```
Using 32 threads
PC has 16 physical cores
Malloc cost = 0.006423
init mmap file cost = 0.012093ms
Parallel process file cost = 426.903ms
Aggregate stats cost = 1.62528ms
Output stats cost = 0.732153ms
Runtime inside main = 429.37ms
Time to munmap = 152.399
Time to free memory = 2.74905
real	0m0.587s
user	0m12.367s
sys	0m0.874s

Using 8 threads
PC has 16 physical cores
Malloc cost = 0.006713
init mmap file cost = 0.012985ms
Parallel process file cost = 1194.83ms
Aggregate stats cost = 1.18586ms
Output stats cost = 0.726166ms
Runtime inside main = 1196.85ms
Time to munmap = 155.822
Time to free memory = 0.988442
real	0m1.356s
user	0m9.066s
sys	0m0.505s

Using 1 threads
PC has 16 physical cores
Malloc cost = 0.006593
init mmap file cost = 0.013386ms
Parallel process file cost = 8996.62ms
Aggregate stats cost = 0.189279ms
Output stats cost = 0.72348ms
Runtime inside main = 8997.63ms
Time to munmap = 152.486
Time to free memory = 0.153601
real	0m9.153s
user	0m8.762s
sys	0m0.384s
```

**AMD 2950X**, test version `1brc_valid20`, **10k keys dataset**
This version intentionally avoid hyper threading when it detects high key collision. This trick is often found in math library, for example a matrix multiplication code will scan for special properties in the inputs and select different math algorithms based on that.
```
Using 32 threads
PC has 16 physical cores
Malloc cost = 0.006522
init mmap file cost = 0.013446ms
n_threads = 16
Gather key stats cost = 8.58899
Parallel process file cost = 1341.34ms
Aggregate stats cost = 15.5884ms
Output stats cost = 13.5001ms
Runtime inside main = 1379.14ms
Time to munmap = 369.615
Time to free memory = 10.2765
real	0m1.766s
user	0m19.629s
sys	0m1.424s

Using 8 threads
PC has 16 physical cores
Malloc cost = 0.006853
init mmap file cost = 0.012524ms
n_threads = 8
Gather key stats cost = 0.001763
Parallel process file cost = 1851.97ms
Aggregate stats cost = 13.9248ms
Output stats cost = 12.809ms
Runtime inside main = 1878.83ms
Time to munmap = 377.707
Time to free memory = 6.00988
real	0m2.266s
user	0m13.234s
sys	0m1.251s

Using 1 threads
PC has 16 physical cores
Malloc cost = 0.007184
init mmap file cost = 0.013055ms
n_threads = 1
Gather key stats cost = 0.001433
Parallel process file cost = 12820.1ms
Aggregate stats cost = 2.1569ms
Output stats cost = 13.1197ms
Runtime inside main = 12835.5ms
Time to munmap = 369.764
Time to free memory = 1.02998
real	0m13.210s
user	0m12.189s
sys	0m1.007s
```

**Comparison to other submission**
There's a super fast non-compliant solution at: https://curiouscoding.nl/posts/1brc/ 
It doesn't work with all inputs like required, but is extremely fast and has very creative ideas.

~~Fastest compliant one I found~~ Nvm, just failed a test. It will fail all tests that has hash collision, actually
https://github.com/gunnarmorling/1brc/discussions/46
Code: https://github.com/dannyvankooten/1brc/issues/2

Benchmark by @buybackoff with many other submissions: https://hotforknowledge.com/2024/01/13/1brc-in-dotnet-among-fastest-on-linux-my-optimization-journey/

A few Java results are outdated because their latest version haven't been merged into master branch. The numbers are from this commit: 1bbddaaaf6b9a23bfc715b188f2da8908b81efe8
![image](https://github.com/gunnarmorling/1brc/assets/22568157/40b15e22-1cb1-4368-80e7-083826867704)

**i5 12600K, 3600GB DDR4 dual channel**, test version `1brc_valid16`
It has 6c6t high power cores + 4 low power cores, which explains why 1 -> 8 thread difference is smaller than expected.
**5.72 second with 1 thread, 40% less time than 2950X** It seems Zen1+ is really outdated and shouldn't be used to prototype low-level optimization in 2024
```
Using 16 threads
real	0m0.827s
user	0m10.355s
sys	0m0.667s

Using 8 threads
real	0m1.256s
user	0m6.881s
sys	0m0.440s

Using 1 threads
real	0m6.036s
user	0m5.716s
sys	0m0.320s
```

----------------------
------------------------------------
I also benchmark RAM copy bandwidth to compare more. `copy_bandwidth.cpp` uses threads to copy a large `mmap` array into RAM array multiple times.

**RAM copy bandwidth on AMD 5995WX, 128 threads**
```
Bandwidth = 7.56104e+10 byte/s
```
`Parallel process file cost = 152.155ms` => 90.7 GB / s => **120% RAM copy speed**

**RAM copy bandwidth on AMD 2950X, 32 threads**
```
Bandwidth = 2.42118e+10 byte/s
```
`Parallel process file cost = 426.903ms` => 32.3 GB / s => **133% RAM copy speed !!!**

So we've just parsed and processed 1 billion lines of text faster than just copying it. That's because writing into RAM is slow, and we don't write a lot into RAM because most of our hash bins are inside L1/L2 cache. I should find a better comparison.

**Main ideas:**
------------------------------------
- Unsigned int overflow hashing: cheapest hash method possible.
- SIMD to find separator `;`
- SIMD hashing
- SIMD for string comparison in hash table probing
- Parse number as int instead of float 
- Notice properties of actual data
- + 99% of station names has `length <= 16`, use compiler hint + implement SIMD for this specific case. If length > 16, use a fallback => still meet requirements of `MAX_KEY_LENGTH = 100`
- + `-99.9 <= temperature <= 99.9` guaranteed, use special code using this property
- Use mmap for fast file reading
- Use multithreading for both parsing the file, and aggregating the data
- Other random tricks (intentional ordering of variable assignments)
- **Automatic RAM disk**: the contest assumes the file is in RAM, not disk. This is done automatically on Linux if you read the file using `mmap` once, then run the program again. Without this rule, all solutions will be completely different.

**Others**
~~There's a potential out-of-range-access exploit in the code. It's left as exercise for the reader.~~ 

I optimize the code for hyper threading. For example with a 16c32t CPU, if a change improves performance when running 32 threads but slightly decreases performance at 16, I will keep that change. Disabling HT will increase performance at the same thread count (maybe even 20-25%), so there are some situations where it's the correct choice.